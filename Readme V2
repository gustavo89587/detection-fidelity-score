# Detection Fidelity Score (DFS)

## A framework for engineering trust in detection systems

Detection Fidelity Score (DFS) is a **framework for reasoning about how detection systems behave under real‑world constraints**.

Rather than asking *“does this detection work?”*, DFS asks:

> **Where does this detection degrade, and can we predict, measure, and reason about that degradation before it causes harm?**

DFS positions detection as both:

* an **engineering system** (pipelines, schemas, latency, failure modes), and
* an **architecture of trust** (when a signal deserves human or automated action).

It is intentionally not a product, a SIEM feature, or a prescriptive scorecard.

---

## The DFS Core Model

DFS models detection degradation across **three failure domains**:

* **Loss** — the signal is missing, delayed, or incomplete
* **Distortion** — the signal survives but loses semantic meaning
* **Drift** — the signal breaks as assumptions decay over time

Together, these domains define **explicit trust boundaries** in detection systems.

→ See the full model: `docs/core-model.md`

---

## What DFS produces

DFS does **not** produce a single magic number.

It produces:

* A **degradation profile** across Loss, Distortion, and Drift
* Identified failure modes
* Explicit trust assumptions
* Engineering guidance on where robustness matters most

This allows teams to reason about **detection debt** before it manifests as alert fatigue, blind spots, or unsafe automation.

---

## What DFS is (and is not)

**DFS is:**

* A conceptual and technical framework
* A shared language between detection engineers, SOC analysts, and leadership
* A way to make implicit trust decisions explicit

**DFS is not:**

* A SIEM replacement
* A false‑positive tuning methodology
* A measure of analyst performance or business impact

DFS evaluates **detection mechanics**, not SOC outcomes.

---

## Example: One detection, three failure domains

**Detection:** Suspicious PowerShell execution with encoded commands

### Loss

* Telemetry dropped under endpoint CPU pressure
* Process command line truncated due to field length limits

**Impact:** Entire executions disappear from detection scope

**Trust outcome:** ❌ Cannot trust absence of alerts

---

### Distortion

* Base64 content partially redacted for privacy
* Case normalization alters pattern matching

**Impact:** Detection fires inconsistently and with reduced context

**Trust outcome:** ⚠️ Alerts require high analyst effort to interpret

---

### Drift

* Endpoint agent update changes command‑line field naming
* Detection relies on implicit default normalization behavior

**Impact:** Detection silently stops matching

**Trust outcome:** ❌ Trust decays over time without visibility

---

### DFS Interpretation

This detection is not "bad" — but its **trustworthiness is conditional**:

* Unsafe under Loss
* Fragile under Distortion
* Unstable under Drift

DFS makes those conditions explicit.

---

## First technical artifact: DFS Result Schema (MVP)

The first concrete DFS artifact is **not code execution**, but a shared result contract.

### DFS Result (conceptual schema)

```json
{
  "detection": "powershell_encoded_command",
  "environment": "endpoint_windows",
  "dfs": {
    "loss": {
      "risk": "high",
      "signals": ["telemetry_drop", "field_truncation"]
    },
    "distortion": {
      "risk": "medium",
      "signals": ["redaction", "normalization"]
    },
    "drift": {
      "risk": "high",
      "signals": ["schema_change", "default_behavior"]
    }
  },
  "trust_boundary": "contextual_only",
  "notes": "Do not page on absence; suitable for enrichment only"
}
```

This schema:

* Forces explicit reasoning
* Enables tooling later without locking design early
* Keeps human trust decisions visible

---

## Design principle

> Detection systems should degrade **predictably and measurably**, not silently.

DFS exists to ensure that trust in detection is **designed, not assumed**.

---

## Status

DFS is currently a **framework and methodology**, not a finished implementation.

Future work may include:

* Automated Loss / Distortion / Drift testing
* CI‑based trust regression
* Comparative analysis across detection stacks

Thoughtful disagreement and real‑world feedback are encouraged.
